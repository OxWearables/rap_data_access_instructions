{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# UKB data extraction template\nGenerated for OxWearables group by Alaina Shreves 15 December 2022\n\nUpdated by Alaina Shreves 21 March 2023\n\n\n## Introduction\nThis is a Python notebook walking through a simple example for extracting data using the RAP. \n\n### How to run this notebook\n\nThis notebook should be run in a *Spark in JupyterLab* session. Read how to set up a Spark cluster [here](https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data).\n\nBefore each code chunk, there is text labeled 'edit code' and 'standard code.' Most of the included code is required for extracting UKB data using the RAP. To extract your variables of interest, you can run the chunks with <span style=\"color:blue\">blue 'standard code'</span> as is and update the chunks with <span style=\"color:red\">red 'edit code'</span> accordingly.\n\n## Set up the session\n\n### Load modules and set up the Spark session (<span style=\"color:blue\">standard code</span>)", "metadata": {}}, {"cell_type": "code", "source": "import pyspark\nimport dxpy # tools starting with 'dx' are from the DNANexus ecosystem\nimport dxdata\nfrom pyspark.sql.functions import when, concat_ws\nfrom re import sub", "metadata": {}, "execution_count": 2, "outputs": []}, {"cell_type": "code", "source": "sc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)", "metadata": {}, "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": "Note - If you are trying to pull a large dataset and have issues with the session timing out, you can increase the memory (from the default buffer size of 64m) and increase the maxResultSize. You can run the following code instad of the first lines that set up the Spark sesion in two chunks. \n\n***\n// Chunk 1\nimport pyspark\nimport dxpy\nimport dxdata\nfrom pyspark.sql.functions import when\nfrom re import sub\nsc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)\n\n\n// Chunk 2\nfrom pyspark.sql import SparkSession\nconf = spark.sparkContext._conf.setAll([('spark.kryoserializer.buffer.max', '1g'), ('spark.driver.maxResultSize', '4g')])\nspark.sparkContext.stop()\nspark = SparkSession.builder.config(conf=conf).getOrCreate()", "metadata": {}}, {"cell_type": "markdown", "source": "### Dispense a dataset (<span style=\"color:blue\">standard code</span>)", "metadata": {}}, {"cell_type": "code", "source": "dispensed_database = dxpy.find_one_data_object(\n    classname=\"database\", \n    name=\"app*\", \n    folder=\"/\", \n    name_mode=\"glob\", \n    describe=True)\ndispensed_database_name = dispensed_database[\"describe\"][\"name\"]\n\ndispensed_dataset = dxpy.find_one_data_object(\n    typename=\"Dataset\", \n    name=\"app*.dataset\", \n    folder=\"/\", \n    name_mode=\"glob\")\ndispensed_dataset_id = dispensed_dataset[\"id\"]", "metadata": {"trusted": true}, "execution_count": 1, "outputs": [{"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-1-419963cee6bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dispensed_database = dxpy.find_one_data_object(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mclassname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"database\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"app*\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mname_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"glob\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'dxpy' is not defined"], "ename": "NameError", "evalue": "name 'dxpy' is not defined", "output_type": "error"}]}, {"cell_type": "markdown", "source": "### Load dataset (<span style=\"color:blue\">standard code</span>)", "metadata": {}}, {"cell_type": "code", "source": "dataset = dxdata.load_dataset(id=dispensed_dataset_id)", "metadata": {}, "execution_count": 5, "outputs": []}, {"cell_type": "markdown", "source": "### Load tabular participant data (<span style=\"color:blue\">standard code</span> if using main database table, <span style=\"color:red\">edit code</span> if using different database table)\n\nThe below example pulls data from the main database table, called 'particpant.' You might need to access other database tables, like death data or HES data, for your analysis. You can check which database table holds your variables of interest (i.e., 'hesin' contains hospital records and 'death' contains death records) [here](https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data).* Examples of pulling data from other database tables can be found in the first notebook of the [RAP Wearables tutorial](https://github.com/OxWearables/rap_wearables). ", "metadata": {}}, {"cell_type": "code", "source": "#If extracting your own data, change the database table name ('participant') as appropriate.\n\nparticipant = dataset[\"participant\"]", "metadata": {}, "execution_count": 6, "outputs": []}, {"cell_type": "markdown", "source": "Next, we'll load a list of fields we might want for our analysis.\n\n### Load utility function (<span style=\"color:blue\">standard code</span>)", "metadata": {}}, {"cell_type": "code", "source": "def load_column_list(file_name):\n    \"\"\"Load list of UK Biobank column IDs from file\n    Column IDs can be obtained from the UK Biobank showcase: https://biobank.ndph.ox.ac.uk/showcase/index.cgi\n    e.g. 90012 refers to the recommended variable for accelerometer measured physical activity\n        https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=90012\n        \n    This function is due to Aiden Doherty.\n\n    :param str file_name: Name of file listing UK Biobank column IDs\n    :return: list of IDs\n    :rtype: list\n    :Example:\n    >>> load_column_list(\"field_list.txt\")\n    [\"31\", \"34\", \"52\" ...]\n    \"\"\"\n    \n    column_IDs = []\n    \n    with open(file_name) as f:\n            for line in f:\n                li = line.strip()\n                if \"#\" in li:\n                    li = li.split(\"#\")[0].strip()\n                if not li.startswith(\"#\") and not li==\"\":\n                    column_IDs += [li]\n    return column_IDs", "metadata": {}, "execution_count": 7, "outputs": []}, {"cell_type": "markdown", "source": "## Read in column list\n\nThere are two main ways of reading in columns names:\n- Using a candidate field list\n- Directly in the Jupyter notebook\n\nWe present examples of both methods below. \n\n## Use a candidate field list to read in column names (<span style=\"color:red\">edit code</span> to map to column list file)", "metadata": {}}, {"cell_type": "code", "source": "#If extracting your own data, change the column list file location accordingly. \ncolumn_list = load_column_list(\"/mnt/project/wearables-main/data_access_instructions/candidate_field_list_updated_example.txt\") # This is a text file listing the field IDs we'll use", "metadata": {}, "execution_count": 8, "outputs": []}, {"cell_type": "markdown", "source": "## Assign field names\n### Use field numbers to work out the [field *names*](https://dnanexus.gitbook.io/uk-biobank-rap/frequently-asked-questions#how-are-column-names-determined-for-the-dispensed-database) <span style=\"color:blue\">(standard code)</span>\n\n*Note - read more about how UK Biobank fields sometimes have *instance* and *array* indices, which are [indicated in the field names](https://dnanexus.gitbook.io/uk-biobank-rap/frequently-asked-questions#how-are-column-names-determined-for-the-dispensed-database).*", "metadata": {}}, {"cell_type": "code", "source": "def fields_for_id(field_id):\n    from distutils.version import LooseVersion\n    field_id = str(field_id)\n    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n    return sorted(fields, key=lambda f: LooseVersion(f.name))\n\ndef field_names_for_id_instanced(field_id, instance=\"i0\", include_non_instanced=True):\n    candidate_fields = [f.name for f in fields_for_id(field_id)]\n    return_fields = [f for f in candidate_fields if instance in f]\n    if (include_non_instanced): # This means fields without instancing (e.g. sex) will be included, and defaults to true\n        return_fields += [f for f in candidate_fields if \"_i\" not in f]\n    return return_fields", "metadata": {}, "execution_count": 9, "outputs": []}, {"cell_type": "markdown", "source": "## Describe how to treat variables from instance 0 or with no instancing (<span style=\"color:blue\">standard code</span>)", "metadata": {}}, {"cell_type": "code", "source": "field_list = [\"eid\"]\nfor col in column_list:\n    field_list += field_names_for_id_instanced(col, instance=\"i0\")", "metadata": {}, "execution_count": 10, "outputs": []}, {"cell_type": "markdown", "source": "## Add any additional columns not listed in field name file (<span style=\"color:red\">edit code</span> to add any additional columns fields)\n\nThe notation for column names is p(data-field #)_i(instance #). Column IDs can be obtained from the [UK Biobank showcase](https://biobank.ndph.ox.ac.uk/showcase/search.cgi). For example, p6150_i0 refers to ['Vascular/heart problems diagnosed by doctor'](https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=6150) at baseline. ", "metadata": {}}, {"cell_type": "code", "source": "#If extracting your own data, change the name of the database table accordingly. If you are not using data from a field name file, you can use the below line to pull all variables manually, change the \"+=\" symbol to \"=\".\n#In this example, we'll add three variables including \"p6150_i0\": \"self_report_cvd_baseline\", \"p6150_i1\": \"self_report_cvd_inst_1\", and \"p6150_i2\": \"self_report_cvd_inst_2\". \n\nfield_list += [\"p6150_i0\",\"p6150_i1\", \"p6150_i2\"] ", "metadata": {}, "execution_count": 39, "outputs": []}, {"cell_type": "markdown", "source": "## Rename variables (<span style=\"color:red\">edit code</span> to add any additional column names)\nRenaming variables is optional, but it can make the extraction simplier to reference in the future since the variable names can be complex. \n\n*Note - We use two different approaches here to illustrate them- you can use either, neither (i.e. use the fields with their original field names), or a mix.*", "metadata": {}}, {"cell_type": "code", "source": "#If extracting your own data, change the field codes and variable alias names accordingly.\n\n# We'll handcraft some of the names to slot neatly into later code\nfield_list_aliases = {\n    \"eid\" : \"eid\", \n    \"p31\" :  \"sex\",\n    \"p52\" : \"month_birth\", \n    \"p34\" : \"year_birth\", \n    \"p6150_i0\": \"self_report_cvd_baseline\", \n    \"p6150_i1\": \"self_report_cvd_inst_1\", \n    \"p6150_i2\": \"self_report_cvd_inst_2\"\n    }\n\n# We'll then auto-add names for other fields (This is not necessary in this very simple example, but could be helpful in your code where you pull many variables)\ndataset_fields = [participant.find_field(name=x) for x in field_list] # get the full field info, not just the name\nfor field in dataset_fields:\n    if field.name not in field_list_aliases.keys():\n        field_list_aliases[field.name] = field.title", "metadata": {}, "execution_count": 40, "outputs": []}, {"cell_type": "markdown", "source": "## Retrieve relevant data (<span style=\"color:blue\">standard code</span>)", "metadata": {}}, {"cell_type": "code", "source": "participant_data_example = participant.retrieve_fields(names=field_list, engine=dxdata.connect(), coding_values=\"replace\", column_aliases=field_list_aliases)", "metadata": {}, "execution_count": 41, "outputs": []}, {"cell_type": "markdown", "source": "*Note - Setting coding values to \"replace\" in this function means we get variables' values (rather than the coded format they are stored in).*", "metadata": {}}, {"cell_type": "markdown", "source": "## Write the data out as a csv file\n\n### Convert array variables to strings (<span style=\"color:red\">edit code</span> to list all variables that are arrays)\n\nThe Jupyter notebook has a specific way of dealing with array variables. For any 'select all that apply' questions (also called 'arrays'), the notebook will only pull the first selected option unless you spaecifically tell it to pull all columns. In this example, someone could have selected multiple responses to the question asking about 'vascular/heart problems diagnosed by doctor' and we are interested in pulling all possible responses. ", "metadata": {}}, {"cell_type": "code", "source": "#If extracting your own data, ensure that the list of array variables is complete. The csv file will not be generated if you fail to list all array variables. \narrayed_cols = [\"self_report_cvd_baseline\", \"self_report_cvd_inst_1\", \"self_report_cvd_inst_2\"]\n\nfor col in arrayed_cols:\n    participant_data_example = participant_data_example.withColumn(col, concat_ws(\"|\", col))", "metadata": {}, "execution_count": 42, "outputs": []}, {"cell_type": "markdown", "source": "### Spark stores and works with the data in several parts, so we coalesce them into one part (<span style=\"color:blue\">standard code</span>)", "metadata": {}}, {"cell_type": "code", "source": "participant_data_example = participant_data_example.coalesce(1)", "metadata": {}, "execution_count": 43, "outputs": []}, {"cell_type": "code", "source": "#This is a good time to pause and test your data. If you are interested, comment out the below line to see if your dataframe is filled with expected data\n#(participant_data_example.show())", "metadata": {}, "execution_count": 44, "outputs": []}, {"cell_type": "markdown", "source": "# Write data to a csv file", "metadata": {}}, {"cell_type": "markdown", "source": "## Set options for writing to a csv file  (<span style=\"color:blue\">standard code</span>)", "metadata": {}}, {"cell_type": "code", "source": "participant_data_example.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"participant_data_example\")", "metadata": {}, "execution_count": 45, "outputs": []}, {"cell_type": "markdown", "source": "## Upload file to temporary storage (<span style=\"color:blue\">standard code</span>)\nYou can check this ended up in the right place by looking in the folder tab on the left side of your screen above the tab labeled 'DNAnexus'", "metadata": {}}, {"cell_type": "code", "source": "%%bash\nhdfs dfs -copyToLocal /user/root/participant_data_example participant_data_example # this gets it out of the hdfs", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Upload file to your permanent storage (<span style=\"color:red\">edit code</span> with your file name and file path)\nNote - If extracting your own data, change the destination of the output file or the text that follows the \"--dest\" in the second line.", "metadata": {}}, {"cell_type": "code", "source": "%%bash\ndx upload participant_data_example/*.csv --dest data_access_instructions/participant_data_example.csv # this uploads to the permanent storage on the RAP", "metadata": {}, "execution_count": 46, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ID                    file-GK7XFqQJB42JZqj26V1G88X9\n\nClass                 file\n\nProject               project-GJzvzj8JB427k9GB192018x3\n\nFolder                /data_access_instructions\n\nName                  participant_data_example.csv\n\nState                 closing\n\nVisibility            visible\n\nTypes                 -\n\nProperties            -\n\nTags                  -\n\nOutgoing links        -\n\nCreated               Tue Dec  6 11:22:22 2022\n\nCreated by            ahshreves\n\n via the job          job-GK7V86jJB428P6zv6567fKYq\n\nLast modified         Tue Dec  6 11:22:24 2022\n\nMedia type            \n\narchivalState         \"live\"\n\ncloudAccount          \"cloudaccount-dnanexus\"\n"}, {"name": "stderr", "output_type": "stream", "text": "SLF4J: Class path contains multiple SLF4J bindings.\n\nSLF4J: Found binding in [jar:file:/cluster/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n\nSLF4J: Found binding in [jar:file:/cluster/dnax/jars/dnanexus-api-0.1.0-SNAPSHOT-jar-with-dependencies.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n\n22/12/06 11:22:20 WARN metrics.MetricsReporter: Unable to initialize metrics scraping configurations from hive-site.xml. Message:InputStream cannot be null\n\n22/12/06 11:22:20 WARN service.DNAxApiSvc: Using default configurations. Unable to find dnanexus.conf.location=null\n\n22/12/06 11:22:20 INFO service.DNAxApiSvc: apiserver connection-pool config. MaxPoolSize=10, MaxPoolPerRoute=10,MaxWaitTimeout=60000\n\n22/12/06 11:22:20 INFO service.DNAxApiSvc: initializing http connection manager pools\n\n22/12/06 11:22:21 INFO service.DNAxApiSvc: Worker process - IdleConnectionMonitorThread disabled\n\n22/12/06 11:22:21 INFO service.DNAxApiSvc: Worker process - IdleConnectionMonitorThread disabled\n\n22/12/06 11:22:21 INFO service.DNAxApiSvc: initializing DNAxApiSvc\n\n22/12/06 11:22:21 WARN service.DNAxApiSvc: Shutting down Runtime service for Connection Pools\n\n22/12/06 11:22:21 INFO service.DNAxApiSvc: shutting down httpClientConnManager\n\n22/12/06 11:22:21 INFO service.DNAxApiSvc: shutting down httpsClientConnManager\n"}]}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}
